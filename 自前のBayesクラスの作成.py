# -*- coding: utf-8 -*-
"""7_bayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wa3U_8_oNazDkhN5F-MFlL7NnxRsS3LD
"""

import numpy as np
from sklearn.naive_bayes import CategoricalNB

# CategoricalNB用のスムージングパラメータ
ALPHA = 1e-10

# 演習用訓練データ
train_X = np.array([
    [0, 0, 1],
    [0, 0, 1],
    [0, 0, 1],
    [0, 1, 0],
    [0, 1, 1],
    [1, 0, 0],
    [1, 0, 1],
    [1, 1, 0],
    [1, 1, 0],
    [1, 1, 1],
])
train_y = np.array([
    0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
])

# 演習用テストデータ
test_X = np.array([
    [0, 0, 0],
    [0, 0, 1],
    [0, 1, 1],
    [1, 0, 0],
    [1, 1, 0],
])
test_y = np.array([
    0, 0, 0, 1, 1,
])

import numpy as np

class MyBayes:
    def __init__(self):
        # メンバ変数の一覧を宣言
        self.prior_probs = None
        self.post_probs = None

    def fit(self, train_X, train_y):
        # 特徴ベクトルの総数，次元数，及び正解クラスの数を取得
        n = len(train_X)
        d = len(train_X[0])
        c = len(np.unique(train_y))

        # 各クラスの事前確率を計算
        self.prior_probs = np.bincount(train_y) / n
        # 各クラスにおける各次元(カテゴリ)の事後確率を計算
        self.post_probs = []
        for i in range(c):
            # クラスiの特徴ベクトル及びその総数を抽出
            # クラスiのパターンのみの配列.
            x_i = train_X[train_y == i]
            x_len = len(x_i)
            # 各次元（カテゴリ）における事後確率を計算
            class_i_probs = []
            # j次元目だけに注目する.
            for j in range(d):
              x_ij = x_i.T[j]
              # カテゴリごとの事後確率のリスト.
              category_probs = []
              for k in range(c):
                    category_probs.append(np.count_nonzero(x_ij == k) / x_len)
              class_i_probs.append(category_probs)
            self.post_probs.append(class_i_probs)

    # 評価用のメソッド.
    def predict(self, x_array):
        # 結果をいれるリスト
        answer = []
        # それぞれのパターンに対して評価を行う.
        for x in x_array:
            # 個々のパターンの評価自体は_predict_singleで行う.
            answer.append(self._predict_single(x))
        print( np.array(answer))
        # 結果を入れたリストをNumPyの配列に変換して返す.
        return np.array(answer)

    def _predict_single(self, test_X):
        # 特徴ベクトルの次元数，及び正解クラスの数を取得
        d = len(test_X)
        c = len(self.prior_probs)
        box = []
        # 各特徴ベクトルの各クラスに対する事後確率を計算
        for i in range(0, c):
           zigo = self.prior_probs[i]
           for j in range(d):
             zigo *= self.post_probs[i][j][test_X[j]]
           box.append(zigo)
        # 確率が最大のクラスを割り当て，分類結果として返す     
        return np.argmax(box)

# モデルを準備
models = []
models.append(MyBayes())
models.append(CategoricalNB(alpha=ALPHA))

# 各モデルを学習
for model in models:
    model.fit(train_X, train_y)

# 分類結果を取得
results = []
for model in models:
    results.append(model.predict(test_X))

# 分類結果の比較
for result in results:
    print(result == test_y)

